# 경력기술서 항목 50선 — 박규민 (주)유비

> 각 항목은 독립 모듈로 작성되어 있으며, 지원 회사의 JD에 따라 선별·조합하여 사용합니다.
> 항목 간 상호 참조(→ B-01, → D-06 등)를 따라가면 하나의 스토리라인이 형성됩니다.
> 하단 **포지션별 스토리라인 조합 가이드**를 참고하세요.

---

## A. 아키텍처 설계

**[A-01. Spring Boot 기반 레거시 CMS 리빌딩 아키텍처 설계]**

JSP 기반 레거시 CMS는 프론트·백이 단일 WAR로 결합되어 있어 부분 배포가 불가능하고, UI 변경에도 전체 서비스 재시작이 필요했습니다. Spring Boot 3.x(Jakarta EE)로의 전환도 검토했으나, 기존 레거시 프레임워크가 javax.* 패키지에 의존하여 호환이 불가능했습니다. 최종적으로 Spring Boot 2.7 LTS + Nuxt 3 조합을 선택하여 REST API와 SPA를 분리하되, 레거시 프레임워크 라이브러리(커스텀 Exception, TraceHandler 등)는 래퍼 클래스로 격리하여 점진적 탈의존을 가능하게 설계했습니다.

레거시 TraceHandler와 Spring Boot의 GlobalExceptionHandler가 예외 처리 체인에서 충돌하는 문제가 발생했습니다. CustomTraceHandler를 도입하여 레거시 예외를 Spring 예외로 변환하는 브릿지를 구성했으며, `@Order` 어노테이션으로 핸들러 우선순위를 명시하여 해결했습니다. Trade-off로, 레거시 의존성을 완전히 제거하지 못한 채 래퍼로 격리한 구조이므로 유지보수 시 두 프레임워크의 동작을 모두 이해해야 하는 부담이 남습니다.

그 결과 프론트엔드와 백엔드의 독립 배포가 가능해져 UI 변경 시 백엔드 재시작 없이 즉시 반영되었습니다. 이 분리 구조는 이후 JWT 인증(→ B-01), Elasticsearch 검색(→ D-01), SSR/CSR 하이브리드(→ E-01) 등 모든 기능 확장의 기반이 되었으며, Spring Boot 3.x + Jakarta EE 전환 시에도 래퍼 계층만 교체하면 되도록 마이그레이션 경로를 확보해두었습니다.

---

**[A-02. 단일 인스턴스 멀티사이트 아키텍처 설계]**

하나의 CMS 인스턴스에서 복수의 독립 사이트(메뉴·콘텐츠·권한·레이아웃 모두 분리)를 운영해야 했습니다. DB 분리 방식(사이트별 스키마)과 단일 DB 멀티테넌시를 비교했으며, 운영 서버가 단일 장비인 환경에서 DB를 복제하는 것은 비용과 복잡도가 과도하다고 판단하여 단일 DB + hostname 컬럼 기반 필터링을 선택했습니다.

모든 API 요청에 `X-Site-Hostname` 헤더를 주입하여 사이트 컨텍스트를 전달하고, 백엔드 Site 엔티티의 `siteHostName` 필드로 데이터를 격리합니다. Trade-off로 모든 쿼리에 hostname 필터가 필수가 되어 쿼리 누락 시 데이터 누출 위험이 있으며, 이를 방지하기 위해 QueryDSL 기본 조건에 hostname 필터를 포함하는 패턴을 수립했습니다. 프론트엔드에서는 레이아웃 미들웨어(→ E-02)가 사이트명을 slugify하여 레이아웃을 자동 선택합니다.

이 구조를 통해 신규 사이트 추가 시 코드 변경 없이 DB 레코드와 프론트엔드 레이아웃 디렉토리만 추가하면 되어, 사이트 확장 비용을 최소화했습니다. 향후 사이트 수가 증가하여 단일 DB 성능 한계에 도달할 경우, hostname 기반 샤딩 또는 읽기 복제본 분리로 전환할 수 있는 구조입니다.

---

**[A-03. 외부 의존성 격리를 위한 포트-어댑터 패턴 적용]**

SSO 연동, 폐쇄망 IP 정책 등 외부 시스템 의존성이 비즈니스 로직에 직접 침투하면, 연동 방식 변경 시 서비스 전체를 수정해야 하는 문제가 있었습니다. 완전한 Hexagonal 아키텍처 적용은 CMS 규모 대비 과도한 추상화라고 판단하여, 외부 연동 지점만 선별하여 포트-어댑터 패턴을 적용했습니다.

`SiteAccessChecker`를 인터페이스(포트)로 정의하고, `SiteAccessCheckerImpl`이 CIDR 매칭을 수행하는 어댑터로 구현했습니다. 이 패턴을 과도하게 확산하면 단순한 내부 호출까지 인터페이스-구현체 쌍이 생겨 코드량만 늘어나므로, "외부 시스템 의존" 여부를 기준으로 적용 범위를 제한했습니다. Trouble Shooting으로, 초기에 SiteAccessChecker를 모든 컨트롤러에서 직접 호출하다가 JwtAuthenticationFilter로 통합 이동하여 중복 호출을 제거했습니다.

그 결과, SSO 연동 방식이 변경되거나 IP 정책이 WAF로 이관될 때 어댑터만 교체하면 비즈니스 로직은 수정 없이 유지됩니다. 향후 외부 인증 체계(OAuth2, SAML) 추가 시에도 동일 패턴으로 확장이 가능합니다.

---

**[A-04. 통합 API 응답 규격 설계 (ApiResponse\<T\>)]**

API 응답 포맷이 엔드포인트마다 상이하여 프론트엔드에서 성공/실패 분기가 일관되지 않았습니다. Spring HATEOAS도 검토했으나, CMS 프론트엔드가 하이퍼미디어 링크를 활용하지 않는 구조여서 불필요한 복잡도를 추가할 뿐이었습니다. `ApiResponse\<T\>` 제네릭 래퍼 클래스를 도입하여 `success(data)`, `error(code, message)` 두 팩토리 메서드로 전체 응답을 통합했습니다.

`GlobalExceptionHandler`에서 400/401/403/404/409/413/500 HTTP 상태별로 `ApiResponse\<Void\>`를 반환하도록 매핑했습니다. Trouble Shooting으로, 레거시 프레임워크의 커스텀 Exception이 Spring의 예외 핸들러보다 먼저 처리되어 래핑되지 않은 응답이 반환되는 문제가 있었으며, `@Order(Ordered.HIGHEST_PRECEDENCE)`로 GlobalExceptionHandler 우선순위를 최상위로 설정하여 해결했습니다.

이 규격 덕분에 프론트엔드 safeFetch(→ E-03)가 단일 에러 처리 로직으로 50개 이상의 API를 커버할 수 있게 되었고, 신규 API 추가 시에도 응답 파싱 코드를 작성할 필요가 없어졌습니다.

---

**[A-05. 프로파일 기반 환경 분리 설계]**

기존에는 `application.yml` 하나에 로컬/운영 설정을 주석 토글로 관리하여, 주석 해제를 잊고 배포하면 운영 DB에 로컬 설정이 적용되는 사고 위험이 있었습니다. Spring Cloud Config Server 도입도 검토했으나, 단일 서버 운영 환경에서 별도 Config Server를 유지하는 것은 과잉 인프라라고 판단하여 프로파일 기반 분리를 선택했습니다.

`application.yml`(공통) + `application-local.yml` + `application-prod.yml`로 분리하고, `SPRING_PROFILES_ACTIVE` 환경변수만으로 전환합니다. Trouble Shooting으로, YAML에서 `!!`이나 `##`을 포함한 비밀번호가 YAML 태그나 주석으로 파싱되어 인증 실패가 반복되었으며, 특수문자 비밀번호를 반드시 작은따옴표로 감싸도록 컨벤션을 수립하여 해결했습니다. Docker 환경에서는 `switch-env.sh` 스크립트로 `.env.local`/`.env.remote` 전환을 단일 명령으로 수행합니다.

이 구조를 통해 환경 설정 실수로 인한 배포 사고를 원천 차단했으며, 신규 환경(staging 등) 추가 시 `application-staging.yml`만 생성하면 됩니다. 프론트엔드도 동일하게 `NUXT_PUBLIC_API_BASE` 환경변수로 API 엔드포인트를 전환하여 백·프론트 환경 전략을 일관되게 유지합니다.

---

**[A-06. 이벤트 기반 느슨한 결합 구조 설계]**

CMS 콘텐츠 CRUD와 Elasticsearch 색인 동기화가 직접 결합되면, ES 장애 시 CMS 핵심 기능(콘텐츠 저장)까지 실패하는 구조적 문제가 있었습니다. Kafka 도입을 검토했으나, 단일 서버 환경에서 Kafka + ZooKeeper 클러스터를 운영하는 것은 인프라 부담이 과도하여 Spring Events + Transactional Outbox 패턴을 선택했습니다. 단, 이벤트 스키마(contentType, contentIdx, action, timestamp)는 Kafka 메시지 포맷과 호환되도록 설계했습니다.

콘텐츠 변경 시 `IndexingEvent`를 발행하면, `@TransactionalEventListener`가 동일 트랜잭션 내에서 `cms_search_outbox` 테이블에 이벤트를 기록합니다. `@Async IndexingEventListener`가 비동기로 ES 색인을 수행하고, 실패 시 `OutboxPollingService`가 최대 3회 재시도합니다(→ D-06). Trade-off로, Spring Events는 단일 JVM 내에서만 동작하므로 다중 인스턴스 배포 시 이벤트 중복 처리 가능성이 있으며, 이는 Kafka 전환 시 해소됩니다.

CMS 코어 서비스에는 이벤트 발행 한 줄만 추가되어 검색 기능과의 관심사가 완전히 분리되었습니다. 트래픽이 증가하여 다중 인스턴스 운영이 필요해지면, EventListener를 KafkaConsumer로 교체하고 Outbox를 Kafka Producer로 전환하는 것만으로 마이그레이션이 가능합니다.

---

## B. 백엔드 보안

**[B-01. JWT + HttpOnly Cookie 기반 이중 토큰 인증 시스템 구현]**

기존 세션 기반 인증은 서버 메모리에 상태를 보관하므로 수평 확장 시 세션 클러스터링이 필요했습니다. JWT를 Authorization 헤더로 전달하는 방식도 검토했으나, 토큰이 JavaScript에서 접근 가능하여 XSS 공격 시 탈취 위험이 있었습니다. HttpOnly + Secure + SameSite 쿠키로 토큰을 전달하는 방식을 선택하여 XSS를 통한 토큰 접근을 원천 차단했습니다.

ACCESS(15분) / REFRESH(24시간) 이중 토큰 구조를 구현하고, Redis 세션과 결합하여 서버 측 세션 철회(강제 로그아웃)도 가능하게 했습니다. `JwtAuthenticationFilter`가 매 요청마다 Redis 세션 존재 여부를 확인하고 슬라이딩 TTL을 갱신합니다. Trouble Shooting으로, Redis 장애 시 전체 인증이 마비되는 문제가 발견되어 `RedisHealthChecker` 기반 폴백을 구현했습니다. Redis가 다운되면 JWT의 자체 exp 클레임(1시간 연장)으로 인증을 유지하되, 세션 철회 기능은 비활성화됩니다.

Trade-off로, 쿠키 기반 인증은 CSRF 공격에 취약하나 SameSite=Strict 속성으로 대체 방어합니다(→ B-07). 이 인증 구조는 프론트엔드의 safeFetch(→ E-03)와 크로스탭 동기화(→ E-09)의 기반이 되며, OAuth2/OIDC 확장 시에도 토큰 발급 로직만 교체하면 필터 체인은 그대로 유지됩니다.

---

**[B-02. PBKDF2 기반 비밀번호 해싱 및 KISA 가이드라인 준수]**

비밀번호 해싱 알고리즘으로 BCrypt, Argon2, PBKDF2를 비교 검토했습니다. BCrypt는 adaptive cost factor로 GPU 병렬 공격에 강하고, Argon2는 메모리 하드 특성으로 가장 안전하지만, 기존 보안 모듈이 PBKDF2를 기본 지원하며, KISA 암호이용 가이드라인에서도 PBKDF2를 권장 알고리즘으로 명시하고 있어 PBKDF2를 선택했습니다.

`Pbkdf2PasswordEncoder`를 185,000 iterations, 256bit 출력으로 설정하여 NIST SP 800-132 권장 수준을 충족합니다. Trade-off로, PBKDF2는 BCrypt 대비 GPU 병렬 공격에 상대적으로 취약하나, iteration 수를 충분히 높여 단일 해시 계산에 ~200ms가 소요되도록 보정했습니다. 이 설정은 로그인 응답 시간에 영향을 주지만, 인증 요청 빈도가 낮은 CMS 특성상 수용 가능한 수준입니다.

그 결과, 보안 감사 항목(비밀번호 저장 방식)을 충족하면서도 실질적인 보안 강도를 확보했습니다. 향후 레거시 프레임워크 탈의존 시 Argon2id로 전환할 수 있도록, PasswordEncoder를 Bean으로 분리하여 교체 지점을 명확히 해두었습니다.

---

**[B-03. AES-GCM 기반 개인정보 컬럼 레벨 암호화 (JPA Converter)]**

개인정보보호법에 따라 전화번호, 이메일, 주소 등 개인식별정보의 DB 저장 시 암호화가 필수였습니다. AES-CBC와 AES-GCM을 비교했으며, CBC는 암호화만 제공하는 반면 GCM은 인증 태그를 통해 암호문 변조 감지까지 가능하여 AES-GCM을 선택했습니다.

`CryptoUtil`이 AES/GCM/NoPadding 알고리즘, 12바이트 랜덤 IV, 128비트 인증 태그를 사용하며, `StringCryptoConverter` JPA Converter를 통해 `@Convert` 어노테이션만 부착하면 엔티티 필드가 자동으로 암호화/복호화됩니다. 암호키는 별도 보안 디렉토리의 `crypto.key`에서 hex 포맷으로 로드하며, Docker 환경에서는 read-only 볼륨으로 마운트하여 변조를 방지합니다. Trouble Shooting으로, 로컬과 운영 환경의 키 파일 경로가 달라 복호화 실패가 발생하여, 프로파일별 `crypto.key.path`를 분리 설정했습니다.

Trade-off로, 컬럼 레벨 암호화는 DB에서 직접 LIKE 검색이 불가능해지는 한계가 있으며, 암호화 대상 필드의 검색이 필요한 경우 별도 해시 인덱스 컬럼을 추가해야 합니다. 현재 CMS에서는 개인정보 필드 검색이 관리자 화면의 정확 일치 조회로 제한되어 이 방식이 적합합니다.

---

**[B-04. HTML Sanitization AOP를 활용한 이중 XSS 방어]**

게시판 HTML 에디터를 통한 XSS 공격을 방어해야 했으나, 모든 서비스 메서드에 개별적으로 정제 로직을 삽입하면 누락 위험과 코드 중복이 발생합니다. Servlet Filter 레벨에서 일괄 정제하는 방식도 검토했으나, HTML 에디터의 정상적인 태그까지 제거되는 오탐 문제가 있어 서비스 레벨 AOP를 선택했습니다.

`@SanitizeHtml` 커스텀 어노테이션과 `HtmlSanitizerAspect`를 구현하여, `@Transactional` 메서드 진입 시 DTO의 HTML 필드를 자동 정제합니다. `HtmlSanitizerUtil`은 `\<script\>`, `javascript:`, `vbscript:`, `on*=` 이벤트 핸들러, CSS `expression()`, `\<iframe\>`, `\<form\>`, `\<object\>` 등 위험 패턴을 제거하되, `\<p\>`, `\<strong\>`, `\<img\>` 등 에디터 허용 태그는 화이트리스트로 유지합니다. 프론트엔드에서도 `SafeHtml.vue` 컴포넌트가 isomorphic-dompurify로 렌더링 시점에 이중 방어합니다.

이 구조를 통해 백엔드(저장 시)와 프론트엔드(렌더링 시) 양단에서 XSS를 차단하며, 신규 서비스 메서드 추가 시 `@SanitizeHtml`만 부착하면 별도 정제 코드 없이 방어가 적용됩니다.

---

**[B-05. CIDR 지원 IP 기반 접근 제어 시스템]**

사이트별 접근 IP를 제한해야 했으나, 별도 WAF 장비가 없는 환경이어서 애플리케이션 레벨에서 처리해야 했습니다. 단순 IP 목록 매칭으로는 서브넷 단위 제어가 불가능하여, CIDR 표기법(`172.16.0.0/12`)을 지원하는 `IpUtil`을 구현하고 `SiteAccessCheckerImpl`에서 Site 엔티티의 `allowIp`/`denyIp` 정책을 평가합니다.

Trouble Shooting으로, Docker Compose 환경에서 컨테이너 간 통신 IP(172.x.x.x)가 차단되어 서비스가 기동 직후 자기 자신의 헬스체크에 실패하는 문제가 발생했습니다. `172.16.0.0/12`, `192.168.0.0/16` 대역을 로컬 프로파일의 허용 IP에 추가하여 해결했습니다. 프론트엔드 미들웨어(→ E-02)에서도 SSR 단계에 IP 체크를 수행하고, 차단 시 `__ipBlock=1` 쿠키를 설정하여 CSR에서도 동일 차단을 유지합니다.

향후 WAF 도입 시 애플리케이션 레벨 IP 제어를 비활성화하고 WAF로 이관할 수 있으며, 포트-어댑터 패턴(→ A-03)에 의해 `SiteAccessChecker` 구현체만 교체하면 됩니다.

---

**[B-06. 계층형 권한 모델 및 Redis 캐싱 기반 실시간 권한 해소]**

메뉴 단위 접근 권한을 `userLevel`(역할 기반)과 `userIdx`(개인 기반) 이중 기준으로 해소해야 했습니다. 순수 RBAC만으로는 특정 사용자에게만 예외 권한을 부여할 수 없고, ABAC는 CMS 규모에 비해 과도한 복잡도를 추가합니다. RBAC를 기반으로 하되, 개인 오버라이드를 허용하는 하이브리드 모델을 설계했습니다.

`PermissionResolverService`가 `MenuPermissionData`를 Redis에 캐싱하여, 개인 권한이 존재하면 우선 적용하고 없으면 레벨 권한으로 fallback합니다. Trouble Shooting으로, 초기 구현에서 메뉴 트리를 재귀 탐색하여 권한을 해소했는데, 메뉴 깊이가 4단계 이상인 경우 조회 시간이 200ms를 초과했습니다. 메뉴-권한 조인을 단일 쿼리로 변경하고 결과를 Redis에 캐싱하여 응답 속도를 20ms 이하로 10배 이상 개선했습니다.

이 권한 모델은 검색 결과 필터링(→ D-04), 프론트엔드 렌더 권한 체크(→ E-02), 관리자 접근 제어에 일관되게 적용되며, 신규 권한 유형(부서 기반, 조건부 등) 추가 시 `PermissionResolverService`의 해소 로직만 확장하면 됩니다.

---

**[B-07. CORS·CSRF·보안 헤더 통합 정책 설정]**

REST API의 Stateless 특성과 쿠키 기반 JWT 인증(→ B-01)을 동시에 만족하는 보안 정책이 필요했습니다. 전통적인 CSRF 토큰 방식은 SPA에서 토큰 관리가 복잡하고, CORS를 완전 개방하면 크로스오리진 공격에 노출됩니다.

CORS는 프로파일별로 허용 오리진을 한정(local: `localhost:3000`, prod: `cms.itid.co.kr`)하고, CSRF는 SameSite=Strict 쿠키 속성으로 대체 방어합니다. 추가로 X-Frame-Options: DENY(클릭재킹 차단), X-Content-Type-Options: nosniff(MIME 스니핑 방지), HSTS 31536000초(HTTPS 강제), Content-Security-Policy(외부 스크립트 차단)를 설정했습니다. Swagger UI는 운영 환경에서 Nginx IP 화이트리스트로 접근을 제한하여 API 문서 노출을 방지합니다.

Trade-off로, SameSite=Strict는 외부 사이트에서의 링크 클릭 시 쿠키가 전송되지 않아 로그인 상태가 유지되지 않는 제약이 있으나, CMS는 외부 링크 유입이 없는 관리 시스템이므로 보안 우선으로 Strict를 유지합니다.

---

## C. 데이터 접근 전략

**[C-01. JPA + QueryDSL 조합의 타입 안전 동적 쿼리 구현]**
초기 개발 속도를 향상시키기 위해 정적 CRUD에는 Spring Data JPA, 관리자 화면의 다중 조건 검색에서는 QueryDSL로 구현하였습니다.

JPA + QueryDSL 조합은 CRUD 도메인(사이트, 회원, 게시판 관리, 콘텐츠)에 적용하고, JPA 매핑이 불가능한 도메인인 동적 게시판은 JDBC Template으로 분리했습니다.

이를 통해 각각의 도메인 별 데이터 처리 효율을 극대화 시켰으며 다중 조건 검색 개발 시 컴파일 타임 검증과 가독성을 확보하였습니다. 

---

**[C-02. JDBC Template 기반 동적 게시판 CRUD 엔진 — 대용량 테이블 분리 전략]**

실제 운영 중인 클라이언트의 레거시 CMS에서 정적 게시판 단일 테이블에 20만 건 이상의 게시글이 축적되어, 테이블 복사(마이그레이션)가 타임아웃으로 실패하고 LIKE 검색 응답이 수 초 이상 소요되는 성능 병목이 발생했습니다. 근본 원인은 모든 게시판 유형의 데이터가 하나의 테이블에 집중되는 정적 구조였습니다. 이를 해결하기 위해 게시판 유형별로 독립 테이블을 런타임에 생성하는 동적 게시판 구조를 설계했습니다. JPA 엔티티 매핑은 컴파일 타임에 결정되므로 런타임 동적 테이블에 적용이 불가능하고, MyBatis XML도 테이블 구조 자체가 동적이어서 정적 매핑 파일로는 대응할 수 없어, JDBC Template 기반 동적 SQL 빌더를 선택했습니다.

`DynamicBoardSqlBuilder`가 `BoardFieldDefinition` 메타데이터를 기반으로 INSERT, SELECT, UPDATE, DELETE SQL을 런타임에 조합하고, `NamedParameterJdbcTemplate`으로 실행합니다. SQL Injection 방지를 위해 모든 컬럼명과 타입은 화이트리스트 검증을 거치며, 사용자 입력값은 `:parameter` 바인딩으로만 전달합니다. 테이블 분리를 통해 게시판별 데이터 규모가 관리 가능한 수준으로 유지되어 단일 테이블 20만 건 병목이 해소되었고, 각 테이블에 최적화된 인덱스를 독립 적용할 수 있게 되었습니다.

이 엔진을 통해 신규 게시판 유형 추가 시 코드 변경 없이 관리자 UI에서 필드를 정의하면 즉시 사용 가능한 게시판이 생성됩니다. 프론트엔드 테마 렌더링 엔진(→ E-06)과 결합하여 완전한 동적 게시판 시스템을 구현했으며, Elasticsearch 색인(→ D-01, D-07)과 연계하여 분산된 동적 테이블의 데이터를 통합 검색할 수 있는 구조를 확보했습니다.

---

**[C-03. 런타임 DDL 엔진 — 관리자 UI 기반 스키마 변경]**

동적 게시판(→ C-02)의 필드를 관리자가 웹에서 추가·수정·삭제하면 실제 DB 테이블 스키마가 변경되어야 합니다. Hibernate의 `hbm2ddl`은 엔티티 기반 자동 스키마 생성만 지원하고, 런타임 DDL 실행에는 적합하지 않습니다.

`BoardMasterServiceImpl`이 `BoardFieldDefinitionsUpsertRequest`를 수신하면, `BoardSchemaDao`가 `ALTER TABLE ADD/MODIFY/DROP COLUMN` DDL을 동적 생성·실행합니다. SQL Injection 방지를 위해 컬럼명은 `^[a-z_][a-z0-9_]*$` 정규식, 타입은 허용 목록(VARCHAR, INT, TEXT, DATE 등)으로 검증합니다. 스키마 변경 이력은 `BoardMasterSnapshot`으로 기록하여 변경 전 상태를 추적할 수 있습니다.

Trade-off로, 런타임 DDL은 대량 데이터가 있는 테이블에서 ALTER TABLE 실행 시 잠금(lock)이 발생할 수 있으며, 이를 완화하기 위해 변경 전 레코드 수를 확인하고 경고를 표시합니다. 향후 스키마 변경이 빈번해지면 EAV(Entity-Attribute-Value) 패턴으로의 전환도 고려할 수 있습니다.

---

**[C-04. MapStruct 기반 DTO-Entity 컴파일 타임 매핑]**

DTO와 Entity 변환을 수동으로 작성하면 필드 누락 버그가 빈번하고, ModelMapper 같은 리플렉션 기반 라이브러리는 런타임 오버헤드와 타입 안전성 부재가 문제였습니다. MapStruct는 컴파일 타임에 매핑 코드를 생성하여 런타임 비용이 없고, 필드 누락 시 컴파일 에러로 즉시 감지됩니다.

9개의 MapStruct 매퍼(`BoardMasterMapper`, `ContentMapper`, `MenuMapper`, `MemberMapper`, `SiteMapper` 등)를 구현했습니다. `@Mapping` 어노테이션으로 필드명이 다른 경우의 명시적 매핑, `@BeanMapping(nullValuePropertyMappingStrategy = IGNORE)`으로 부분 업데이트를 처리합니다. Spring Component 모델(`componentModel = "spring"`)로 DI 통합했습니다.

Trade-off로, MapStruct는 러닝 커브가 있고 복잡한 변환(조건부, 중첩 객체)에서는 `@AfterMapping` 커스텀 로직이 필요하지만, DTO-Entity 변환이 빈번한 CMS 구조에서 얻는 타입 안전성과 성능 이점이 이를 상회합니다.

---

**[C-05. Redis + 인메모리 이중 캐싱 전략 설계]**

캐싱 대상에 따라 적절한 캐시 계층을 선택해야 했습니다. Redis 단일 캐시로 통합하면 구현이 단순하지만, 변경 빈도가 매우 낮은 데이터(게시판 마스터 설정)까지 네트워크 왕복이 발생하여 불필요한 지연이 추가됩니다.

Redis는 세션(`sessionRedisTemplate`)과 권한 캐시(`redisTemplate`)처럼 다중 인스턴스 간 일관성이 필요한 데이터에 사용하고, 게시판 마스터 설정처럼 변경 빈도가 낮은 데이터는 `ConcurrentMapCacheManager` 인메모리 캐시를 사용합니다. `RedisConfig`에서 `GenericJackson2JsonRedisSerializer`를 적용하여 Redis CLI로 직접 데이터를 확인할 수 있는 가독성을 확보했습니다. `RedisHealthChecker`가 주기적으로 상태를 확인하고, 장애 시 JWT TTL을 연장하여 인증 가용성을 유지합니다(→ B-01).

Trade-off로, 인메모리 캐시는 인스턴스 간 동기화가 불가능하여 다중 인스턴스 배포 시 캐시 불일치가 발생할 수 있습니다. 현재 단일 인스턴스 운영에서는 문제없으며, 다중 인스턴스 전환 시 Redis로 통합하거나 캐시 무효화 이벤트를 브로드캐스트하는 구조로 확장할 수 있습니다.

---

**[C-06. JPA Auditing 기반 자동 감사 필드 관리]**

모든 엔티티의 생성일시(`createdDate`)와 수정일시(`updatedDate`)를 수동으로 세팅하면 서비스 로직마다 `entity.setCreatedDate(LocalDateTime.now())` 같은 중복 코드가 발생하고, 누락 시 감사 추적이 불가능해집니다.

`@EnableJpaAuditing` + `@CreatedDate` / `@LastModifiedDate` 어노테이션으로 JPA 이벤트 리스너가 자동 처리하도록 구성했습니다. `AuditingEntityListener`가 persist 시 생성일시를, merge 시 수정일시를 자동 기록합니다. Trade-off로, JPA Auditing은 JDBC Template으로 직접 INSERT하는 동적 게시판(→ C-02)에는 적용되지 않으며, 해당 영역은 SQL Builder에서 `NOW()` 함수를 직접 포함합니다.

이를 통해 엔티티 15개 이상에서 감사 필드 관리 코드를 완전히 제거하고, 일관된 시간 기록을 보장합니다.

---

## D. Elasticsearch 검색

**[D-01. Nori 한국어 분석기 기반 풀텍스트 검색 도입 — 대용량 동적 테이블 통합 검색]**

레거시 CMS에서 정적 게시판 단일 테이블 20만 건 이상의 데이터에 MySQL LIKE 검색을 수행하면 응답이 수 초 이상 소요되었고, FULLTEXT 인덱스로 전환해도 한국어 형태소 분석이 미지원되어 "시스템 관리"로 검색해도 "시스템관리자"가 매칭되지 않았습니다. 동적 게시판 구조(→ C-02)로 테이블을 분리한 후에는 30종 이상의 분산 테이블을 통합 검색해야 하는 새로운 과제가 생겼습니다. Elasticsearch + Nori 분석기를 도입하여 분산 테이블의 데이터를 단일 ES 인덱스에 통합 색인하고, 한국어 형태소 분석 기반 풀텍스트 검색을 구현했습니다. Solr도 검토했으나, Spring Data Elasticsearch의 성숙도와 Kibana 모니터링의 운영 편의성이 우수하여 ES를 선택했습니다.

`IndexSetupServiceImpl`이 Nori 분석기(`nori_tokenizer` + `nori_part_of_speech` 불용어 필터)를 포함한 커스텀 인덱스를 생성합니다. 색인 시 콘텐츠 텍스트뿐 아니라 `visibility`(공개/비공개), `siteHostName`(멀티사이트 식별), `menuId`(권한 필터링 키)까지 다차원으로 색인하여, 검색 시 텍스트 매칭 + 권한 필터(→ D-04) + 사이트 필터를 단일 쿼리로 처리합니다. Trouble Shooting으로, Nori 플러그인이 ES 버전과 정확히 일치해야 하며, Docker에서는 커스텀 이미지(→ G-02)로, 운영 서버에서는 수동 설치로 대응합니다.

이 구조를 통해 20만 건 이상의 데이터에서도 한국어 형태소 기반 검색이 밀리초 단위로 응답하며, 동적 게시판의 테이블 분산 구조를 사용자에게 투명하게 통합 검색으로 제공합니다. 향후 동의어 사전 추가나 자동완성(suggest) 기능 확장이 Nori 분석기 설정 변경만으로 가능합니다.

---

**[D-02. BM25 알고리즘 기반 검색 랭킹 및 필드 가중치 적용]**

단순 매칭으로는 검색 결과의 관련성 순서를 보장할 수 없어, 게시판 제목에 키워드가 포함된 결과가 본문에만 포함된 결과보다 상위에 노출되어야 하는 요구사항이 있었습니다.

`SearchServiceImpl`에서 `multi_match` 쿼리로 `title^2`(제목 가중치 2배) + `body` 필드를 대상으로 BM25 랭킹 검색을 수행합니다. 하이라이팅 기능으로 검색어 매칭 부분을 `\<strong\>` 태그로 감싸 200자 이내 스니펫을 생성합니다. TF-IDF 기반 커스텀 스코어링도 검토했으나, BM25가 문서 길이 정규화를 기본 포함하여 본문이 긴 게시글과 짧은 메뉴 페이지 간 공정한 랭킹이 가능하여 기본 BM25를 유지했습니다.

Trade-off로, 필드 가중치(boost)는 정적 설정이므로 사용자 행동 기반 동적 랭킹은 지원하지 않습니다. 향후 클릭률 데이터가 축적되면 function_score 쿼리로 동적 부스팅을 추가할 수 있습니다.

---

**[D-03. Circuit Breaker 기반 검색 서비스 장애 격리]**

Elasticsearch는 CMS의 부가 기능이므로 ES 장애가 CMS 핵심 기능(콘텐츠 작성·조회)에 영향을 주어서는 안 됩니다. 단순 try-catch로 예외를 잡으면 매 요청마다 ES 연결 타임아웃(30초)을 기다려야 하고, 장애 복구 후 자동 재연결 로직도 없습니다.

Resilience4j `@CircuitBreaker`를 `SearchService.search()` 메서드에 적용하여, 10회 호출 중 실패율 50% 초과 시 회로를 개방합니다. 개방 상태에서는 ES 호출 없이 즉시 503 fallback을 반환하여 타임아웃 대기를 제거합니다. 30초 후 Half-open 상태에서 3회 시험 호출로 자동 복구를 시도합니다. 프론트엔드의 `SearchServiceDown.vue` 컴포넌트가 서비스 장애 메시지를 표시합니다.

Trouble Shooting으로, 초기에 slow call 임계값(2초)을 너무 짧게 설정하여 정상 인덱싱 중에도 회로가 개방되었습니다. slow call rate 임계값을 80%로 조정하여 일시적 지연과 실제 장애를 구분했습니다. Circuit Breaker 상태가 OPEN에서 복구되지 않는 경우 백엔드 컨테이너 재시작으로 리셋합니다.

---

**[D-04. 2단계 권한 필터링 검색 — Index-time + Query-time]**

검색 결과에 비공개 콘텐츠가 노출되면 "존재 자체"가 정보 누출이 됩니다. ES 인덱싱 시점에만 권한을 필터링하면 사용자별 동적 권한 변경을 반영할 수 없고, 쿼리 시점에만 처리하면 ES에서 불필요하게 많은 결과를 가져와야 합니다.

1단계(Index-time): ES 쿼리에서 `visibility=PUBLIC` 필터로 비공개 콘텐츠를 사전 제거합니다. 2단계(Query-time): `PermissionResolverService.hasPermissionForMenu()`(→ B-06)로 각 검색 결과의 메뉴 접근 권한을 검증하고, 권한 없는 결과를 후처리로 제거합니다. 정확한 페이지네이션을 위해 `PERMISSION_FETCH_BUFFER = 2x`로 충분한 후보를 가져와 필터링 후에도 요청된 페이지 크기를 채울 수 있도록 합니다.

Trade-off로, Query-time 필터링은 추가 연산 비용이 발생하지만, 사용자별 실시간 권한 변경을 즉시 반영할 수 있다는 이점이 이를 상회합니다. 향후 ES 인덱스에 `allowed_user_levels` 필드를 추가하여 1단계 필터링을 강화하면 Query-time 부담을 줄일 수 있습니다.

---

**[D-05. Zero-Downtime 재색인 — Read/Write Alias Swap 전략]**

인덱스 매핑 변경이나 전체 재색인 시 기존 검색 서비스를 중단할 수 없었습니다. ES의 `_reindex` API는 같은 인덱스 내에서 동작하므로 매핑 변경이 불가능하고, 인덱스를 삭제 후 재생성하면 그 사이에 검색이 불가능합니다.

Read Alias(`cms_content_read`)와 Write Alias(`cms_content_write`)를 분리하고, 재색인 시 새 인덱스(`cms_content_v{n+1}`)를 생성하여 배치 색인을 수행합니다. 그동안 기존 인덱스는 Read Alias를 통해 검색 서비스를 계속 제공합니다. 색인 완료 후 `_aliases` API로 Read/Write Alias를 원자적으로 전환하여 다운타임 없이 새 인덱스로 전환합니다.

`ReindexServiceImpl`이 이 전체 흐름을 관리하며, `ConsistencyValidationService`가 재색인 전후의 문서 수를 비교하여 데이터 누락을 검증합니다. 향후 인덱스 규모가 커지면 스냅샷 기반 복원이나 병렬 색인으로 재색인 시간을 단축할 수 있습니다.

---

**[D-06. Transactional Outbox 패턴 기반 색인 일관성 보장]**

동적 게시판 30종 이상의 분산 테이블(→ C-02)에서 CRUD가 발생할 때마다 ES 색인을 동기화해야 하지만, 비동기 이벤트(→ A-06)로 처리하면 이벤트 발행 후 ES 색인 전에 애플리케이션이 재시작될 경우 이벤트가 유실됩니다. 20만 건 이상의 데이터에서 색인 누락은 검색 결과의 신뢰성을 직접 훼손합니다. 메시지 큐(Kafka, RabbitMQ)를 도입하면 해결되지만, 단일 서버 환경에서는 과잉 인프라입니다.

`SearchOutbox` 엔티티(status: PENDING/PUBLISHED/FAILED, retry_count, contentType, contentIdx, action)를 CMS와 동일한 트랜잭션으로 기록하여 이벤트 유실을 방지합니다. 비동기 처리 성공 시 PUBLISHED로 변경하고, 실패 시 `OutboxPollingService`가 `@Scheduled`로 PENDING 레코드를 주기적으로 재시도합니다. 최대 3회 실패 후 FAILED로 마킹하여 수동 복구가 가능합니다. 초기 전체 색인은 `POST /search/admin/setup` API로 `BatchIndexDao`가 JDBC로 500건 단위 Bulk 색인을 수행합니다.

Trouble Shooting으로, date 필드 정렬 시 `missing` 값을 문자열 `"0"`으로 설정하여 타입 불일치 에러가 발생했습니다. ES date 필드의 `missing` 값은 epoch millis(Long)여야 한다는 것을 확인하고 수정했습니다. 또한 range 쿼리에서 `.format()`을 명시하지 않으면 다중 날짜 포맷이 인식되지 않는 문제도 해결했습니다.

---

**[D-07. Elasticsearch 대용량 배치 색인 및 초기 셋업 자동화]**

운영 환경에서 클라이언트의 레거시 데이터 20만 건 이상을 포함한 30종 이상의 동적 게시판 테이블 데이터를 ES로 일괄 마이그레이션해야 했습니다. 수동 색인은 절차 누락과 설정 오류 위험이 높고, 대량 데이터를 한 번에 색인하면 ES 메모리 부족으로 실패합니다.

`POST /search/admin/setup` API 한 번의 호출로 인덱스 생성 → Nori 분석기 매핑 → Read/Write Alias 설정 → 전체 데이터 배치 색인이 순차 수행됩니다. `BatchIndexDaoImpl`이 JDBC로 동적 게시판 테이블까지 포함한 모든 콘텐츠를 추출하되, 각 문서에 `siteHostName`(사이트), `visibility`(공개 상태), `menuId`(권한 키)를 함께 색인하여 검색 시 다차원 필터링이 가능하도록 합니다. `IndexingServiceImpl`이 500건 단위 Bulk API로 메모리 부하를 제어하면서 대량 데이터를 효율적으로 색인하고, 색인 결과(성공/실패 건수, 소요 시간)를 `IndexingResult`로 반환하여 검증합니다.

이후 증분 동기화는 Outbox 패턴(→ D-06)이 자동 처리하며, 매핑 변경이 필요한 경우에만 `/search/admin/reindex` API로 Zero-Downtime 재색인(→ D-05)을 수행합니다. 이 구조를 통해 20만 건 규모의 레거시 데이터 마이그레이션과 일상적 증분 색인을 모두 단일 아키텍처로 처리합니다.

---

## E. 프론트엔드

**[E-01. Nuxt 3 SSR/CSR 하이브리드 렌더링 전략 설계]**

공개 페이지는 SEO와 초기 로딩 속도를 위해 SSR이 필요하고, 관리자 페이지는 인증 필수이므로 CSR이 적합합니다. React(Next.js)도 검토했으나, 프로젝트 기술 스택 검토에서 Vue 생태계의 러닝커브와 SSR 지원 성숙도가 우수하여 Nuxt 3을 선택했습니다. SPA 전용(CSR only)은 SEO 불가, SSR 전용은 관리자 화면의 복잡한 상태 관리에 부적합하여 하이브리드 전략을 설계했습니다.

`init.server.ts` 플러그인에서 정적 리소스 경로(`/contents/`, `/uploads/`, `/_nuxt/`)를 정규식으로 감지하여 SSR 초기화를 스킵합니다. 운영 환경에서는 Nginx(→ G-03)가 정적 리소스를 직접 서빙하므로 이 로직은 개발 환경 가드 역할만 합니다. Trade-off로, SSR은 서버 렌더링 비용이 추가되어 동시 접속 증가 시 Node.js 프로세스 부하가 증가하며, 이를 완화하기 위해 Vue Query 캐시(→ E-04)로 불필요한 SSR 데이터 패칭을 최소화했습니다.

이 하이브리드 구조 위에 5계층 미들웨어(→ E-02), safeFetch(→ E-03), 동적 테마 렌더링(→ E-06)이 구축되어 전체 프론트엔드 아키텍처의 기반이 됩니다.

---

**[E-02. 5계층 미들웨어 파이프라인 설계 — SSR/CSR 브릿지]**

SSR과 CSR에서 동일한 인증·권한·라우팅 로직을 실행해야 하지만, SSR에서는 쿠키·헤더로, CSR에서는 브라우저 API로 정보에 접근하는 방식이 다릅니다. 단일 미들웨어로 모든 로직을 처리하면 조건 분기가 복잡해지고 유지보수가 어렵습니다.

5계층으로 미들웨어를 분리하여 각각 단일 책임을 수행하도록 설계했습니다: `01.ip-access-guard`(IP 차단, SSR에서 판정 후 쿠키로 CSR 전달) → `02.auth-guard`(사이트 개방 상태 + 로그인 리다이렉트) → `03.menu-management`(메뉴 트리 로딩 + 경로 정규화) → `04.render-permission`(렌더 데이터 + 메뉴 권한 체크) → `05.layout-selector`(멀티사이트 레이아웃 자동 선택). Trouble Shooting으로, SSR에서 리다이렉트 후 CSR에서 다시 리다이렉트가 발생하는 무한 루프 문제가 있었으며, SSR에서 1회성 플래그(`useState`)를 설정하여 CSR 중복 실행을 방지했습니다.

이 파이프라인은 멀티사이트 아키텍처(→ A-02)와 계층형 권한 모델(→ B-06)을 프론트엔드에서 실현하는 핵심 구조이며, 신규 미들웨어 추가 시 번호 기반 순서 삽입으로 기존 로직에 영향 없이 확장됩니다.

---

**[E-03. safeFetch 통합 API 레이어 구현 — 인증·캐시·에러 일원화]**

Nuxt 3의 기본 `useFetch`는 인증 헤더 주입, 세션 만료 처리, 에러 통합 게이트 등 CMS에 필요한 기능이 부족합니다. 각 API 호출마다 개별적으로 인증 처리를 구현하면 코드 중복과 누락이 발생합니다.

`ofetch` 래퍼인 `safeFetch`(360+ lines)를 구현하여 전체 API 호출을 일원화했습니다. 주요 기능: 클라이언트 인메모리 요청 캐시, `X-Menu-Id`/`X-Site-Hostname` 헤더 자동 주입, `X-Session-Expires` 기반 자동 로그아웃 타이머, 401/403 통합 인증 게이트(중복 팝업 방지를 위한 플래그 관리), SSR 7초/CSR 2.5초 차등 타임아웃, 실패 시 `defaultValue` fallback. ApiResponse\<T\>(→ A-04) 규격과 정합하여 성공/실패 분기가 단일 패턴으로 처리됩니다.

Trade-off로, 모든 API가 safeFetch를 경유하므로 이 레이어에 장애가 생기면 전체 API가 영향받는 단일 장애 지점(SPOF)이 됩니다. 이를 완화하기 위해 safeFetch 내부에서 예외를 절대 전파하지 않고, 실패 시 `defaultValue` 또는 `null`을 반환하여 호출부가 방어적으로 처리할 수 있도록 설계했습니다.

---

**[E-04. TanStack Vue Query 기반 데이터 캐싱 전략]**

CMS 관리 화면은 사용자가 명시적으로 저장/조회할 때만 데이터가 변경되므로, 자동 refetch는 불필요하고 오히려 편집 중인 데이터가 초기화되는 문제를 일으킵니다. Nuxt의 `useAsyncData`는 SSR 캐싱에 특화되어 있으나 클라이언트 사이드 캐시 관리가 제한적입니다.

Vue Query를 `staleTime: Infinity`, `retry: 0`, `gcTime: 5분`으로 설정하여 명시적 무효화 전까지 캐시를 유지합니다. 모든 자동 refetch(`refetchOnWindowFocus`, `refetchOnReconnect`, `refetchOnMount`)를 비활성화하여 CMS 관리 화면의 예측 가능한 데이터 흐름을 보장합니다. 데이터 변경 후 `queryClient.invalidateQueries()`로 명시적으로 캐시를 갱신합니다.

Trade-off로, 자동 refetch를 비활성화하면 다른 관리자가 변경한 데이터를 실시간으로 반영하지 못하는 한계가 있으나, CMS 관리 작업은 동시 편집 빈도가 낮아 수용 가능합니다. 향후 실시간 협업이 필요해지면 WebSocket 기반 캐시 무효화 이벤트를 추가할 수 있습니다.

---

**[E-05. Pinia 기반 6-스토어 상태 관리 설계]**

Vuex 4는 TypeScript 지원이 미흡하고 mutation/action 분리가 보일러플레이트를 증가시킵니다. Pinia는 Vue 3 공식 상태 관리 라이브러리로 TypeScript-first 설계, Composition API 네이티브, mutation 없는 직관적 API를 제공합니다.

6개 스토어를 설계했습니다: `userStore`(인증 상태), `menuStore`(메뉴 트리 + 브레드크럼), `renderStore`(렌더 메타데이터 + 테마), `searchStore`(검색 결과 + 필터), `siteStore`(멀티사이트 설정), `uiStore`(반응형 UI 상태). `pinia-plugin-persistedstate`로 `userStore`, `siteStore`를 localStorage에 자동 영속화하여 새로고침 시에도 상태를 유지합니다. `authBus`(→ E-09)를 통해 크로스탭 인증 이벤트를 `userStore`에 반영합니다.

스토어 간 의존 관계를 단방향으로 유지(user → menu → render)하여 순환 참조를 방지하고, 각 스토어의 테스트 가능성을 확보했습니다.

---

**[E-06. 동적 게시판 테마 렌더링 엔진]**

30종 이상의 게시판 유형(→ C-02)을 각각 별도 페이지로 만들면 코드 중복이 극심하고, 새 유형 추가마다 라우터 등록이 필요합니다.

`[...slug].vue` catch-all 라우터에서 `renderStore.type`에 따라 `\<Board\>` 또는 `\<Content\>`를 `defineAsyncComponent()`로 코드 스플리팅하여 로드합니다. `Board.vue`는 게시판 유형별 테마 컴포넌트(`theme/list/default/`, `theme/searchBox/`, `theme/pagination/`)를 조합하여 list/view/write 모드를 동적 렌더링합니다. 메뉴 ID + 렌더 데이터 기반 컴포넌트 키로 Vue의 컴포넌트 재사용 캐시 충돌을 방지합니다.

이 구조를 통해 신규 게시판 테마 추가 시 `theme/` 디렉토리에 컴포넌트만 추가하면 되고, 라우터나 빌드 설정 변경이 불필요합니다. 백엔드 동적 게시판(→ C-02)과 결합하여 코드 변경 없이 새 게시판 유형을 즉시 운영할 수 있는 완전한 동적 게시판 시스템을 구현했습니다.

---

**[E-07. 프론트엔드 구조화 로깅 시스템 구현]**

운영 환경에서 `console.log`가 남아 있으면 성능 저하와 정보 노출 위험이 있고, 클라이언트 에러를 서버로 수집하는 기능도 없었습니다. 서드파티 로깅 서비스(Sentry 등) 도입은 폐쇄망 환경에서 외부 통신이 불가능하여 자체 구현이 필요했습니다.

`logger.ts`(280+ lines) 싱글턴 로거를 구현했습니다. 버퍼 기반(최대 1000건), 비동기 서버 전송, 로그 레벨(debug/info/warn/error), 컨텍스트 자동 캡처(userId, sessionId, userAgent, URL, IP), `startTimer()` 성능 측정. 운영 환경에서는 `console.log`/`console.debug`를 noop 함수로 교체하고, `console.warn`/`console.error`만 로거로 리다이렉트합니다.

Trade-off로, 클라이언트 로그의 서버 전송은 네트워크 부하를 추가하므로, 현재는 error 레벨만 전송하고 debug/info는 버퍼에만 보관합니다. 향후 Elastic APM이나 자체 로그 수집 서버를 추가하면 전체 레벨 전송으로 확장할 수 있습니다.

---

**[E-08. 타입 안전 UI 메시지 시스템]**

에러·성공 메시지를 컴포넌트마다 하드코딩하면 메시지 일관성이 깨지고, 오타로 인한 버그가 발생합니다. i18n 라이브러리(vue-i18n)도 검토했으나, 단일 로케일(한국어)만 사용하는 CMS에서는 다국어 인프라가 불필요한 오버헤드였습니다.

`uiMessages.ko.ts`에 한국어 메시지 딕셔너리를 `Record\<UiMessageKey, string\>` 타입으로 정의하고, `formatUiMessage(key, params)`가 `{fieldName}` 플레이스홀더를 치환합니다. TypeScript 키 타입으로 존재하지 않는 메시지 키를 컴파일 타임에 감지합니다. SSR에서 CSR로 메시지를 전달할 때는 `msg` 쿼리 파라미터를 통해 `ssr-msg.client.ts` 플러그인이 처리합니다.

향후 다국어 지원이 필요해지면 `uiMessages.en.ts`를 추가하고 로케일 선택 로직만 삽입하면 되도록, 메시지 키 구조를 다국어 호환으로 설계해두었습니다.

---

**[E-09. 크로스탭 인증 동기화 및 클라이언트 입력 검증]**

여러 탭에서 CMS를 사용하다가 한 탭에서 로그아웃하면 다른 탭은 로그인 상태로 남아 세션 불일치가 발생합니다. 또한, 클라이언트 측 입력 검증이 없으면 불필요한 API 호출이 증가하고 사용자 경험이 저하됩니다.

`authBus.ts`(110 lines)가 BroadcastChannel API(우선) + localStorage storage 이벤트(fallback)를 통해 탭 간 LOGIN/LOGOUT/PROFILE_UPDATED 이벤트를 동기화합니다. 고유 TAB_ID로 자기 메시지를 필터링하여 에코를 방지합니다. `validator.ts`(160+ lines)는 `ValidationType`별 검증(default 50자, text 1000자, html 10000자, email, phone, url)을 제공하고, `sanitizeHtml()`은 isomorphic-dompurify 기반으로 허용 태그/속성을 화이트리스트로 제한합니다. `fileValidator.ts`는 파일 타입/크기를 검증하여 악성 파일 업로드를 사전 차단합니다.

BroadcastChannel과 DOMPurify는 브라우저 호환성에 차이가 있으나, 각각 localStorage fallback과 서버 사이드 정제(→ B-04)로 이중 방어하여 어떤 환경에서도 안전합니다.

---

## F. 웹 접근성 및 UX

**[F-01. KWCAG 2.2 준수 통합 검색 UI 구현]**

대상 웹사이트에 KWCAG(한국형 웹 콘텐츠 접근성 지침) 2.2 준수가 요구사항이며, 검색 페이지는 스크린 리더 사용자가 가장 빈번하게 접근하는 기능 중 하나입니다.

검색 페이지(`/common/search.vue`, 870+ lines)에 웹 접근성을 전면 적용했습니다: `\<form role="search"\>`로 검색 랜드마크 지정, `\<nav role="navigation"\>`으로 페이지네이션 탐색, 모든 필터 그룹에 `aria-label`/`aria-labelledby` 연결, 결과 영역에 `aria-live="polite"`로 동적 업데이트 알림, 검색 후 `nextTick()` 포커스 이동, `role="status"`로 결과 건수 실시간 표시, `.sr-only` 스크린 리더 전용 텍스트, 키보드 네비게이션(Tab/Enter/Arrow), 페이지네이션 `aria-current="page"`. 콘텐츠 타입별 필터는 `role="radiogroup"`으로 시맨틱하게 구성했습니다.

접근성 준수는 검색 UI뿐 아니라 공통 컴포넌트(GnbMenu, Breadcrumb, Sidebar)에도 적용되어, 웹 접근성 인증 심사를 통과할 수 있는 수준을 확보했습니다.

---

**[F-02. 반응형 레이아웃 및 다크모드 멀티 디바이스 대응]**

CMS 관리자가 모바일에서도 긴급 콘텐츠 수정을 할 수 있어야 하며, 장시간 작업 시 눈의 피로를 줄이는 다크모드 요구가 있었습니다. CSS 미디어 쿼리만으로는 SSR 단계에서 디바이스 판별이 불가능하여 서버 렌더링 결과와 클라이언트 렌더링이 불일치(hydration mismatch)합니다.

`@nuxtjs/device` 모듈로 SSR 단계에서 User-Agent 기반 디바이스를 판별하고, `uiStore.isMobile`로 상태를 관리합니다. Tailwind CSS 반응형 클래스로 모바일/태블릿/데스크톱 레이아웃을 전환하며, 모바일에서는 사이드바를 토글 방식으로 변경합니다. `@nuxtjs/color-mode` 모듈과 `ColorModeToggle.vue` 컴포넌트로 라이트/다크 모드를 전환하되, Element Plus의 `dark/css-vars.css`와 Tailwind dark variant를 연동하여 전체 UI가 일관되게 전환됩니다.

Trade-off로, SSR 디바이스 판별은 User-Agent 기반이므로 실제 뷰포트 크기와 차이가 있을 수 있으며, CSR에서 `window.matchMedia`로 보정합니다.

---

## G. DevOps 및 인프라

**[G-01. Docker Compose 7-서비스 멀티컨테이너 개발환경 구성]**

로컬 개발 환경에서 MySQL, Redis, Elasticsearch, Nginx, Backend, Frontend, Kibana를 각각 수동으로 설치·관리하면 환경 차이로 인한 "내 PC에서는 동작하는데" 문제가 빈발합니다. Kubernetes도 검토했으나, 단일 개발자 환경에서 K8s 클러스터를 운영하는 것은 과잉 복잡도입니다.

Docker Compose로 7개 서비스를 단일 `docker-compose.yml`로 관리합니다: MySQL 8.0, Redis 7, Elasticsearch 7.17(커스텀 Nori 이미지, → G-02), Nginx 1.25, Spring Boot(Gradle, 소스 마운트), Nuxt 3(Node 20, HMR), Kibana 7.17. 5개 Named Volume(mysql_data, redis_data, es_data, gradle_cache, frontend_node_modules)으로 컨테이너 재생성 시에도 데이터를 영속화하고, `cms-network` 브릿지 네트워크로 서비스 간 통신을 격리합니다.

`docker compose up -d` 단일 명령으로 전체 개발 환경이 구동되어, 신규 개발자 온보딩 시간을 대폭 단축했습니다. 운영 환경은 Docker를 사용하지 않고 직접 설치(bare-metal)이므로, 프로파일 기반 설정 분리(→ A-05)로 환경 차이를 흡수합니다.

---

**[G-02. 커스텀 Elasticsearch Docker 이미지 — Nori 플러그인 내장]**

공식 Elasticsearch 7.17 Docker 이미지에는 Nori 한국어 분석기 플러그인이 포함되어 있지 않아, 컨테이너 기동 시마다 수동 설치가 필요했습니다. 플러그인 설치에 네트워크 접근이 필요하므로 폐쇄망 환경에서는 동작하지 않습니다.

`docker/elasticsearch/Dockerfile`에서 `FROM elasticsearch:7.17.25` 기반으로 `RUN bin/elasticsearch-plugin install --batch analysis-nori`를 실행하는 커스텀 이미지를 빌드했습니다. 이미지에 플러그인이 내장되어 있으므로 컨테이너 기동 시 추가 설치가 불필요하고, 폐쇄망에서도 사전 빌드된 이미지를 로드하면 됩니다.

운영 서버에서는 Docker를 사용하지 않으므로 `elasticsearch-plugin install analysis-nori` 수동 설치로 동일 환경을 재현하며, 설치 절차를 문서화(→ K-01)하여 재현 가능성을 확보했습니다.

---

**[G-03. Nginx 리버스 프록시 설계 — 7개 라우팅 규칙 및 정적 자원 최적화]**

백엔드(Spring Boot, 8080)와 프론트엔드(Nuxt, 3000)가 분리된 구조에서 클라이언트가 단일 도메인(80/443)으로 접근하려면 리버스 프록시가 필수입니다. Traefik도 검토했으나, 운영 환경에서 이미 Apache/Nginx가 구축되어 있고, 개발 환경에서는 설정 파일 관리가 더 직관적인 Nginx를 선택했습니다.

`cms.conf`(138 lines)에 7개 location 블록을 설계했습니다: `/contents/cms/`(업로드 파일 직접 서빙, 1년 캐시), `/public/`(정적 자원), `/cms/json/`(JSON 데이터), `/swagger-ui/`(백엔드, IP 제한), `/back-api/`(백엔드 프록시), `/static/`(백엔드 정적), `/`(Nuxt 프론트엔드). keepalive 32 커넥션으로 백엔드 연결 재사용, 50MB 업로드 제한, WebSocket 업그레이드(Nuxt HMR 지원), 폰트 파일 CORS 헤더를 설정했습니다.

이 구조를 통해 정적 자원은 Nginx가 직접 서빙하여 백엔드/프론트엔드 서버 부하를 줄이고, 동적 요청만 프록시하여 전체 응답 속도를 개선했습니다.

---

**[G-04. Jenkins CI/CD 파이프라인 — 4단계 + 수동 승인 게이트]**

수동 빌드·배포는 절차 누락과 휴먼 에러 위험이 높고, 배포 이력 추적이 불가능합니다. GitHub Actions도 검토했으나, 사내 Gitea 서버를 사용하는 환경에서 Jenkins가 이미 구축되어 있어 Jenkins를 활용했습니다.

`Jenkinsfile`(83 lines)에 Checkout → Build(`./gradlew clean build -x test`) → Archive Artifact → Deploy 4단계를 구성했습니다. Deploy 단계에 `input "배포를 진행하시겠습니까?"` 수동 승인 게이트를 추가하여 실수 배포를 방지합니다. SCP로 JAR + deploy.sh를 운영 서버에 전송하고 SSH로 원격 실행합니다.

Trouble Shooting으로, 초기 Gitea Webhook에서 Branch Filter가 비어 있어 feature 브랜치 push에도 Jenkins가 트리거되었습니다. Branch Filter를 `main`으로 설정하여 feature push 시에는 빌드가 실행되지 않도록 수정했습니다. 향후 테스트 자동화가 추가되면 Build 단계에 `-x test`를 제거하고 Test 단계를 분리할 수 있습니다.

---

**[G-05. 운영 배포 스크립트 설계]**

Jenkins에서 원격 실행하는 배포 스크립트가 필요하며, 배포 시 기존 서비스를 안전하게 종료하고 새 버전을 기동해야 합니다. systemd 서비스로 관리하면 이상적이지만, 운영 서버 접근 권한 제약으로 systemd 등록이 불가능했습니다.

`deploy.sh`(23 lines)가 `pkill -f 'java -jar'`로 기존 프로세스를 종료하고, `nohup java -jar -Dspring.profiles.active=prod`로 새 버전을 백그라운드 실행합니다. 날짜별 로그 파일(`api_YYYYMMDD.log`)로 분리하여 로그 관리를 용이하게 했습니다.

Trade-off로, `pkill` + `nohup` 방식은 프로세스 종료와 기동 사이에 서비스 공백이 발생합니다. 백엔드는 API 호출 빈도가 상대적으로 낮은 CMS 특성상 수용 가능한 수준이며, 프론트엔드는 이 한계를 해결하기 위해 별도의 Blue/Green 무중단 배포(→ G-08)를 구현했습니다.

---

**[G-06. Git 브랜치 전략 및 Webhook 설정]**

소규모 팀(1-2인)에서 GitFlow(develop/release/hotfix)는 브랜치 관리 오버헤드가 과도합니다. `main`(운영) + `feature/*`(개발) 전략을 선택하여, feature에서 작업 → push → 완료 시 main으로 `--ff-only` 머지 → main push로 Jenkins 트리거하는 흐름을 수립했습니다.

`--ff-only` 머지 정책으로 main의 선형 히스토리를 유지하고, main 직접 커밋을 금지합니다. Gitea Webhook Branch Filter를 `main`으로 설정하여 feature push 시 Jenkins가 트리거되지 않습니다.

이 전략은 팀 규모가 커지면 develop 브랜치를 추가하여 GitFlow로 자연스럽게 확장할 수 있으며, 현재 규모에서는 최소한의 브랜치 관리 비용으로 안정적인 배포 흐름을 유지합니다.

---

**[G-07. WSL2 파일시스템 최적화 및 환경 전환 자동화]**

Windows에서 Docker를 사용할 때 프로젝트가 NTFS 파일시스템(`/mnt/c/`)에 위치하면, Linux 컨테이너의 모든 파일 I/O가 9P 프로토콜을 거쳐 NTFS↔ext4 변환이 발생하여 빌드 속도가 극도로 저하됩니다.

프로젝트를 WSL2 네이티브 파일시스템(`/home/` 하위)으로 이전하고, Docker Compose에서 WSL2 경로를 직접 마운트하여 Gradle 빌드 시간을 5~10배 개선했습니다. 추가로 `switch-env.sh` 스크립트를 구현하여, `local`/`remote` 인자에 따라 `.env.local` 또는 `.env.remote`를 `.env`로 복사하는 환경 전환을 단일 명령으로 수행합니다. local 모드에서 `.env.local`이 없으면 기본값으로 자동 생성하여 초기 설정 누락을 방지합니다.

이 최적화를 통해 개발 사이클(코드 변경 → 빌드 → 테스트)이 대폭 단축되었으며, 환경 전환 실수로 인한 잘못된 DB 연결을 방지했습니다.

---

**[G-08. 프론트엔드 Blue/Green 무중단 배포 파이프라인 설계]**

백엔드 배포 스크립트(→ G-05)는 `pkill` + `nohup` 방식으로 프로세스 교체 시 수 초간 서비스 공백이 발생합니다. 백엔드는 API 호출 빈도가 낮아 수용 가능하지만, 프론트엔드는 사용자가 직접 접속하는 페이지이므로 배포 중 화면이 깨지거나 503이 발생하면 사용자 경험에 즉각 영향을 줍니다. 이를 해결하기 위해 프론트엔드 전용 Blue/Green 배포를 설계했습니다. 또한 CI(빌드)와 CD(배포)를 분리하여, 빌드 아티팩트를 검증한 후 승인 게이트를 거쳐 배포하는 안전한 흐름을 구성했습니다.

CI 파이프라인(`frontend-ci.groovy`)이 `npm ci` → `npm run build` → `npm prune --omit=dev`로 런타임 최소 구성만 `build.tar.gz`로 아카이빙합니다. CD 파이프라인(`frontend-cd.groovy`)은 CI 아티팩트를 받아 운영 서버에 릴리스 디렉토리(`releases/{timestamp}_b{buildNumber}`)를 생성하고, Shared Folder(→ G-09)의 `public/` 디렉토리를 심링크(`ln -sfn`)로 연결한 뒤 새 포트(3001)에서 Node.js를 기동합니다. 60초간 `/api/health/check`와 루트 경로에 Health Check를 수행하여 정상 응답을 확인한 후, Apache의 `nuxt-upstream.conf`에 새 포트를 기록하고 `apachectl graceful`로 트래픽을 무중단 전환합니다. 전환 완료 후 이전 포트의 프로세스를 종료합니다. Trouble Shooting으로, 초기에 Health Check가 루트 경로만 확인하여 API 라우트가 아직 준비되지 않은 상태에서 트래픽이 전환되는 문제가 있었으며, `/api/health/check` 엔드포인트를 추가하여 서버 사이드 라우팅까지 정상 동작을 검증합니다.

이 구조를 통해 프론트엔드 배포 시 사용자에게 서비스 중단 없이 새 버전이 반영됩니다. 실패 시 이전 릴리스 디렉토리가 보존되어 있으므로 `current` 심링크만 롤백하면 즉시 복구가 가능합니다. 향후 백엔드에도 동일 패턴을 적용하여 전체 서비스의 무중단 배포를 달성할 수 있습니다.

---

**[G-09. 통합 Shared Folder 경로 전략 — 빌드 아티팩트와 정적 자원 분리]**

CMS는 업로드 이미지, 사이트 로고, 메뉴 JSON, 첨부파일 등 다양한 정적 자원을 관리하는데, 이를 빌드 아티팩트(JAR, frontend bundle) 안에 포함하면 두 가지 문제가 발생합니다. 첫째, 코드 변경이 없어도 정적 파일이 변경될 때마다 전체 빌드·배포가 필요하고, 둘째 아티팩트 크기가 불필요하게 커져 빌드 시간과 네트워크 전송 시간이 증가합니다. 실제로 업로드 이미지만 수백 MB에 달하는 상황에서 이를 매 배포마다 JAR에 포함시키는 것은 비효율적이었습니다.

모든 정적 자원을 통합 Shared 디렉토리 하나(`shared/`)에 집중시키고, 하위 구조를 `content/`(업로드 파일), `public/`(로고·파비콘), `json/`(메뉴 트리 JSON), `uploads/`(첨부)로 표준화했습니다. 핵심은 **개발(Docker)·운영(bare-metal) 환경에서 동일한 절대경로를 사용**하는 것입니다. 백엔드 `application.yml`의 `paths.shared-root`가 양 환경 모두 같은 경로를 가리키고, 프론트엔드 `nuxt.config.ts`의 `nitro.publicAssets`도 동일 경로를 참조합니다. Docker에서는 볼륨 마운트(`./shared:/path/shared`)로, 운영 서버에서는 디스크 직접 접근으로 이 경로를 보장합니다. 빌드 시 Gradle JAR에서 정적 자원 디렉토리를 제외하고, 프론트엔드 CI(→ G-08)에서도 `public/` 폴더를 아카이브에 포함하지 않고 배포 시 심링크로 연결합니다.

그 결과 백엔드 JAR 크기가 대폭 감소하고, 프론트엔드 빌드 아티팩트도 경량화되어 CI/CD 파이프라인 전체 소요 시간이 단축되었습니다. 정적 파일 변경(로고 교체, 이미지 업로드)은 빌드·재배포 없이 즉시 반영되며, Apache/Nginx가 Shared 디렉토리에서 직접 서빙(→ G-10)하여 애플리케이션 서버에 요청이 도달하지 않습니다. 환경별 경로 분기가 없으므로 "로컬에서 되는데 운영에서 안 되는" 파일 경로 문제를 원천 차단합니다.

---

**[G-10. Apache 리버스 프록시 + HTTPS — 운영서버 정적 자원 직접 서빙]**

개발 환경에서는 Nginx(→ G-03)를 리버스 프록시로 사용하지만, 운영 서버에서는 Apache httpd가 이미 구축되어 있어 운영 환경에 맞는 별도의 리버스 프록시 설정이 필요했습니다. 특히 SSL 인증서 관리, 정적 자원의 디스크 직접 서빙, Blue/Green 배포(→ G-08)의 포트 전환을 Apache 단에서 처리해야 했습니다.

Apache VirtualHost에서 Let's Encrypt SSL 인증서로 HTTPS를 제공하고, Shared Folder(→ G-09)의 정적 자원은 `ProxyPass !` + `Alias`로 프록시를 우회하여 디스크에서 직접 서빙합니다. `/contents/cms/`(업로드 파일), `/public/`(로고), `/cms/json/`(메뉴 JSON) 3개 경로에 `mod_expires`로 1년 immutable 캐시, `X-Content-Type-Options: nosniff` 보안 헤더, ETag 비활성화를 설정했습니다. API 요청(`/back-api/`)은 Spring Boot(8080)로, 프론트엔드(`/`)는 Nuxt.js로 프록시합니다. Blue/Green 배포 시 `Include /etc/httpd/nuxt-upstream.conf` 파일의 포트를 변경하고 `apachectl graceful`로 무중단 전환합니다. Swagger UI는 `LocationMatch`로 특정 IP에서만 접근 가능하도록 화이트리스트를 설정했습니다.

Trade-off로, Apache와 Nginx(Docker)의 설정 파일을 이중 관리해야 하는 부담이 있으나, 개발 환경과 운영 환경의 역할이 명확히 분리되어 있어 설정 충돌은 발생하지 않습니다. 정적 자원이 Apache에서 직접 서빙되므로 Node.js(프론트엔드)와 Spring Boot(백엔드) 서버의 정적 파일 요청 부하가 제거되어 동적 요청 처리에 리소스를 집중할 수 있습니다.

---

## H. AOP 및 횡단 관심사

**[H-01. @ExecutionTime 커스텀 어노테이션 기반 메서드 성능 측정]**

특정 API의 응답 시간이 느린 원인을 파악하려면 메서드 단위 실행 시간 측정이 필요합니다. Spring Actuator + Micrometer로 메트릭을 수집하는 방법도 있으나, 전체 메트릭 인프라 없이 개발 단계에서 간편하게 사용할 수 있는 경량 도구가 필요했습니다.

`@ExecutionTime(description, level, unit)` 커스텀 어노테이션과 `ExecutionTimeAspect` AOP를 구현하여, 어노테이션을 부착한 메서드의 실행 시간을 지정 단위(NANO/MICRO/MILLI/SECONDS)와 로그 레벨(DEBUG/INFO/WARN/ERROR)로 자동 기록합니다. 예외 발생 시에도 실행 시간을 기록하여 예외 처리 비용을 포함한 성능 분석이 가능합니다.

이 도구로 권한 해소 로직의 재귀 탐색 병목(→ B-06)을 발견하여 10배 성능 개선을 이끌었습니다. 향후 Micrometer 도입 시 @ExecutionTime 데이터를 Prometheus 메트릭으로 연동할 수 있습니다.

---

**[H-02. 비동기 감사 로깅 — 로그인 AOP에서 CRUD 전체 감사 + Redis Streams 설계]**

로그인 성공/실패 이력은 보안 감사의 필수 요소이지만, 로깅 로직을 인증 서비스에 직접 삽입하면 비즈니스 로직과 감사 로직이 결합됩니다. 더 나아가, 콘텐츠 생성·수정·삭제, 권한 변경, 회원 관리 등 모든 CRUD 작업의 감사 추적이 필요했으나, 현재 `LoggingUtil`은 애플리케이션 로그(SLF4J)에만 기록하여 DB 검색이 불가능했습니다.

1단계로, `LoginLogAspect`가 인증 서비스의 로그인 성공/실패를 AOP로 감지하여, IP·타임스탬프·사용자 정보를 `@Async`(core=2, max=8 스레드 풀)로 `cms_login_log` 테이블에 JDBC 배치 기록합니다. 2단계(설계 완료)로, 전체 CRUD 감사를 위해 Redis Streams 기반 비동기 파이프라인을 설계했습니다. AOP 기반 `CrudAuditAspect`가 서비스 레이어의 CREATE/UPDATE/DELETE 반환을 감지하여 Redis `XADD`로 감사 이벤트를 발행하고, Consumer Group이 배치로 소비하여 감사 테이블에 기록하는 구조입니다. `@Async` + JDBC 직접 기록 대비 Redis Streams를 선택한 이유는, CRUD 감사 이벤트는 로그인보다 빈도가 수십 배 높아 CMS DB에 직접 쓰기 부하를 주면 핵심 트랜잭션 성능에 영향을 주기 때문입니다. Redis Streams는 순서 보장 + Consumer Group 배치 소비 + 백프레셔를 제공하면서도 Kafka 대비 운영 복잡도가 낮습니다.

Trade-off로, Redis Streams는 Redis 장애 시 감사 이벤트가 유실될 수 있으나, Redis AOF 영속화와 `MINID` 기반 트리밍으로 완화합니다. 향후 감사 데이터가 대규모로 증가하면 Kafka로 전환할 수 있도록, 이벤트 스키마(action, entityType, entityId, userId, timestamp, diff)를 Kafka 호환으로 설계해두었습니다.

---

**[H-03. 방문자 추적 인터셉터 및 통계 스케줄러]**

일별·월별 방문자 통계를 수집해야 하지만, 모든 API 요청을 대상으로 하면 성능 부담이 과도합니다. Filter가 아닌 Interceptor를 선택한 이유는, 특정 경로(`/back-api/auth/me`)에만 선택적으로 적용하여 불필요한 오버헤드를 최소화하기 위함입니다.

`VisitorInterceptor`가 `/back-api/auth/me` 경로에서 방문자 정보를 수집하고, `VisitorServiceImpl`이 일별 방문 통계를 관리합니다. `VisitorMigrationService`가 `@Scheduled`로 방문 데이터를 주기적으로 집계·마이그레이션하여, 실시간 데이터 테이블의 크기를 관리합니다.

이 구조를 통해 CMS 대시보드에서 방문자 추이를 시각화할 수 있으며, 향후 Google Analytics 연동이나 커스텀 대시보드 확장에 데이터 기반을 제공합니다.

---

## I. 메뉴 및 콘텐츠 관리

**[I-01. 메뉴 트리 JSON Export 및 버전 관리 — API 의존 제거]**

프론트엔드가 매 페이지 로드마다 메뉴 API를 호출하면, 백엔드 부하 증가와 초기 로딩 지연이 발생합니다. 메뉴 구조는 변경 빈도가 낮으므로 정적 JSON 파일로 서빙하는 것이 효율적입니다.

`MenuSyncProcessor`가 메뉴 CRUD 시 계층 트리를 JSON 파일(Shared 디렉토리의 `json/`)로 내보내어, 프론트엔드 `menuStore`가 API 호출 없이 정적 JSON에서 메뉴를 로드합니다. `MenuSyncLock`으로 동시 Export를 방지하고, `JsonVersionService`가 버전 번호를 관리하여 프론트엔드 캐시 무효화 시점을 제어합니다.

Trade-off로, JSON 파일 기반은 실시간 반영이 아니라 Export 시점에만 갱신되므로, 메뉴 변경 후 JSON Export를 잊으면 불일치가 발생합니다. 이를 방지하기 위해 `@TransactionalEventListener`로 메뉴 변경 시 자동 Export를 트리거합니다.

---

**[I-02. 드래그앤드롭 메뉴 트리 편집기]**

계층형 메뉴 트리의 순서 변경, 부모 변경을 번호 입력 방식으로 처리하면 직관성이 떨어지고 오류가 빈번합니다.

프론트엔드 `useMenuDragDrop()` composable이 메뉴 트리의 드래그앤드롭 재배치를 구현합니다. `useMenuTreeOperations()`가 CRUD 작업을, `useMenuVersionManager()`가 메뉴 변경 이력을 관리하여 변경 전 상태로 롤백할 수 있습니다. 드래그 앤 드롭 시 계층 깊이 제한, 순환 참조 방지 검증을 수행합니다.

Trade-off로, 커스텀 드래그앤드롭 구현은 서드파티 라이브러리(vue-draggable-next 등) 대비 유지보수 부담이 있으나, 계층 검증·버전 관리·백엔드 동기화까지 통합된 커스텀 로직이 필요하여 자체 구현을 선택했습니다.

---

**[I-03. 콘텐츠 파일 첨부 및 수정 이력 관리]**

콘텐츠와 첨부 파일의 생명주기가 다르므로(콘텐츠 삭제 시 파일도 삭제, 파일만 교체 등) 별도 서비스로 분리해야 합니다. 수정 이력이 없으면 이전 버전으로 롤백이 불가능합니다.

`ContentFileServiceImpl`이 파일 업로드·다운로드·삭제를 처리하고, `useContentFiles()` composable이 프론트엔드 파일 관리를 담당합니다. `useContentHistory()`가 콘텐츠 수정 이력을 관리하여 이전 버전 조회 및 비교가 가능합니다. `useContentPreview()`로 발행 전 미리보기를 제공하여 실수 발행을 방지합니다.

파일 저장 경로는 Shared 디렉토리의 `contents/`에 날짜별 디렉토리로 관리하며, Docker 환경에서는 볼륨 마운트로 컨테이너 재생성 시에도 파일을 유지합니다.

---

## J. LLM 기반 개발 프로세스

**[J-01. LLM 기반 설계·개발 프로세스 도입 및 워크플로 구축]**

AI 기반 개발 역량 확보가 조직 내 요구되었으나, CLI 환경에서 LLM을 실질적인 설계·개발 워크플로에 적용한 경험자가 없어 주도적으로 프로세스 도입을 진행했습니다. BMad 에이전트 프레임워크와 Claude Code CLI를 활용해 요구사항 정의, 아키텍처 설계, API 계약, 이벤트 흐름을 MD 산출물 중심으로 표준화하고, 에이전트·페르소나·스킬 기반 규칙 체계를 수립했습니다.

LLM이 생성한 설계 및 코드 초안은 그대로 채택하지 않고, 도메인 제약 조건, 인터페이스 정합성, 비기능 요구사항(성능·장애·권한) 누락 여부를 점검하는 검증 프로세스를 구축했습니다. 이를 통해 AI 산출물을 단순 보조 도구가 아닌 검증 가능한 설계·개발 입력물로 활용하는 체계를 마련했습니다.

그 결과, 설계 산출물 작성 리드타임을 단축하고 인터페이스 변경에 따른 재작업을 감소시켰으며, 설계 의사결정 일관성을 확보하여 신규 기능 확장 시 안정적인 구조 확장을 가능하게 했습니다.

---

**[J-02. AI 산출물 품질 검증 체계 및 산출물 체인 관리]**

LLM이 생성한 설계 문서는 표면적으로 완결되어 보이지만, 도메인 특수 제약(레거시 호환성, 보안 기준)을 누락하거나 인터페이스 간 불일치가 숨어 있는 경우가 많습니다.

Product Brief → PRD(46 FR + 35 NFR) → Architecture → Epics(6개) → Stories(21개) → Implementation Artifacts(18개)의 산출물 체인을 수립하고, `implementation-readiness-report.md` 검증 프로세스로 46 FR 전체 매핑, Epic 품질, UX 정합성을 체크리스트로 검증합니다. READY FOR IMPLEMENTATION 판정을 받은 후에만 구현에 착수하는 게이트를 운영했습니다.

이 체계를 통해 설계-구현 간 갭을 최소화하고, LLM 산출물에 대한 맹목적 신뢰를 방지하여 실질적인 AI 활용 역량을 확보했습니다. 모든 산출물은 Markdown 기반으로 Git 버전 관리되어 변경 이력 추적이 가능합니다.

---

## K. 문서화 및 팀 기여

**[K-01. 기술 문서 체계 구축 — 아키텍처·검색·환경·보안]**

코드만으로는 아키텍처 의사결정의 배경과 대안 검토 과정을 파악할 수 없어, 인수인계 시 지식 전달이 어렵고 동일한 의사결정을 반복하게 됩니다.

`architecture.md`(데이터 전략, 보안 전략, API 설계, 인프라 전략, 네이밍 규칙), `ES_SEARCH_INTEGRATION.md`(19KB, 인덱스 설계, 콘텐츠 매핑, 검색 API 스펙, 데이터 일관성 전략, 권한 필터링), `ENVIRONMENT_SETUP.md`(프로파일 설정, Docker 셋업, 환경 전환), `SECURITY.md`(10KB, 보안 경계, 인증 통합, 권한 모델) 등 9개 기술 문서를 체계적으로 작성했습니다.

이 문서 체계를 통해 신규 팀원 온보딩 시 프로젝트 구조를 독립적으로 파악할 수 있는 기반을 마련하고, 보안 감사 시 참조 문서로 활용할 수 있도록 했습니다.

---

**[K-02. 미들웨어 파이프라인 및 운영 매뉴얼 문서화]**

프론트엔드 5계층 미들웨어(→ E-02)는 SSR/CSR 하이브리드의 복잡한 실행 흐름을 포함하고 있어, 코드만으로는 전체 동작을 이해하기 어렵습니다. 또한 운영 환경의 ES 초기 셋업, Nori 플러그인 설치, 배포 절차 등도 문서화가 필요했습니다.

`middleware/README.md`에 5계층 미들웨어의 실행 순서, 각 미들웨어의 기능·특징·SSR/CSR 대응 방식, 전체 설계 철학(관심사 분리, redirect loop 방지, 멀티사이트 최적화)을 문서화했습니다. ES 운영 매뉴얼에는 Nori 플러그인 설치, 인덱스 초기 셋업 API 호출, 재색인 절차, Circuit Breaker 리셋 방법을 단계별로 기록했습니다.

이 문서들은 BMad 산출물 체인(→ J-02)의 일부로 Git 버전 관리되며, 코드 변경 시 관련 문서도 함께 업데이트하는 규칙을 수립하여 문서-코드 간 불일치를 방지합니다.

---

## 포지션별 스토리라인 조합 가이드

> 아래 조합에서 항목을 선별하여 경력기술서를 구성하면, 서론에서 언급한 키워드가 본문에서 수미상관으로 이어지는 흐름이 형성됩니다.

### 백엔드 중심 포지션
**서론 키워드**: "레거시 모놀리스를 REST API 기반으로 리빌딩하면서, 보안·데이터·검색의 관심사를 분리한 구조를 설계했습니다"
**추천 흐름**: A-01(아키텍처) → A-04(API 규격) → B-01(JWT 인증) → B-03(암호화) → B-06(권한) → C-01(JPA+QueryDSL) → C-02(동적 게시판) → D-01(ES 검색) → D-06(Outbox) → H-01(성능 측정)

### 풀스택 포지션
**서론 키워드**: "Spring Boot + Nuxt 3 분리 아키텍처에서 SSR/CSR 하이브리드와 통합 보안을 구현했습니다"
**추천 흐름**: A-01(아키텍처) → A-02(멀티사이트) → B-01(JWT) → E-01(SSR/CSR) → E-02(미들웨어) → E-03(safeFetch) → E-06(테마 엔진) → F-01(접근성) → G-01(Docker) → K-01(문서화)

### DevOps / 인프라 포지션
**서론 키워드**: "CI/CD 파이프라인 설계부터 Blue/Green 무중단 배포, 정적 자원 분리를 통한 빌드 최적화까지 전체 배포 라이프사이클을 구축했습니다"
**추천 흐름**: A-05(환경 분리) → G-01(Docker Compose) → G-04(Jenkins) → G-08(Blue/Green 무중단 배포) → G-09(Shared Folder 전략) → G-10(Apache 리버스 프록시) → G-03(Nginx) → G-05(배포 스크립트) → G-06(Git 전략) → D-05(Zero-Downtime 재색인)

### 검색 / 데이터 엔지니어링 포지션
**서론 키워드**: "Elasticsearch + Nori 기반 한국어 풀텍스트 검색을 설계하고, 장애 격리와 데이터 일관성을 확보했습니다"
**추천 흐름**: D-01(Nori 검색) → D-02(BM25 랭킹) → D-03(Circuit Breaker) → D-04(권한 필터링) → D-05(Alias Swap) → D-06(Outbox) → D-07(배치 색인) → A-06(이벤트 기반) → C-05(Redis 캐싱) → G-02(커스텀 ES 이미지)

### AI / LLM 활용 포지션
**서론 키워드**: "LLM을 설계·개발 워크플로에 실질적으로 통합하고, 산출물 검증 체계를 구축했습니다"
**추천 흐름**: J-01(LLM 프로세스) → J-02(산출물 검증) → A-01(아키텍처 설계) → A-06(이벤트 기반) → D-01(ES 검색) → E-02(미들웨어) → K-01(문서 체계) → K-02(운영 매뉴얼)
